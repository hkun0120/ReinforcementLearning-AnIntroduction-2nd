# 📘 Reinforcement Learning (2nd Edition) — Preface to the Second Edition

**Authors:** Richard S. Sutton & Andrew G. Barto  
**Translator:** kun hong  
**Purpose:** Non-commercial educational translation and study notes  
**License:** CC BY-NC-SA 4.0  
**Original Book:** Reinforcement Learning: An Introduction, Second Edition (2018)

---

## 🇨🇳 中文翻译 — 第二版序言

自本书第一版出版以来的二十年间，人工智能领域取得了飞跃性的发展。这一进步在很大程度上归功于机器学习的重大突破，其中包括强化学习的进展。虽然强大的计算能力为部分成果的实现提供了基础，但新的理论与算法的发展同样是驱动力。面对这样的变化，我们1998年的版本早已显得“姗姗来迟”，因此我们终于在2012年启动了第二版的编写。我们的目标与第一版一致：**以清晰、简明的方式阐述强化学习的关键思想与算法，让所有相关领域的读者都能轻松理解。**

本书依旧是一本**入门教材（introduction）**，重点聚焦于核心的**在线学习算法（online learning algorithms）**。新版本中包含了过去二十年间兴起的重要主题，并扩展了我们如今理解更深入的部分。然而，我们并未尝试覆盖整个领域——强化学习已经在许多方向上蓬勃发展。对于不得不舍弃的大量贡献，我们深表歉意。

与第一版一样，我们选择不对强化学习进行严格的形式化处理，也没有用最一般的框架来表达。然而，我们对某些主题的理解更为深入，因此需要使用更多数学来解释。为此，我们将数学性较强的部分放入**灰色阴影框**中，读者可选择性跳过。

在符号方面，本版略有调整。在教学中我们发现，这样的修改能减少许多常见的混淆。我们特别强调**随机变量（用大写字母表示）**与其**具体取值（用小写字母表示）**之间的区别。例如，时间步 *t* 的状态、动作与奖励分别表示为 **St、At、Rt**，其可能的取值分别为 **s、a、r**。  
同样地，价值函数采用小写（如 \(v^{\pi}\)），而其表格型估计值采用大写（如 \(Q_t(s,a)\)）。  
近似价值函数（approximate value function）是随机参数的确定性函数，因此也使用小写，例如 \( \hat{v}(s, w_t) \approx v^{\pi}(s) \)。  
向量（如权重向量 **w_t**，旧称 **θ_t**，以及特征向量 **x_t**，旧称 **φ_t**）均以**粗体小写**表示，即便它们是随机变量；**粗体大写**仅用于矩阵。

在第一版中，我们使用了 \(P^a_{ss'}\) 与 \(R^a_{ss'}\) 表示转移概率与期望奖励。该记号存在两点不足：其一，仅刻画了奖励的期望而非完整动态过程；其二，下标与上标过多，导致复杂。为此，本版采用新的记法 **p(s′, r | s, a)**，明确表达在当前状态 s、动作 a 下，下一状态 s′ 与奖励 r 的**联合概率分布**。所有符号变更汇总于第 xix 页的符号表中。

---

### 📖 书籍结构与新内容

第二版在篇幅与结构上都有显著扩展。除开篇导论外，全书分为三大部分：

**第一部分（第 2–8 章）**：  
尽量在**表格型情形（tabular case）**下讲述强化学习的核心思想，即能找到精确解的场景。内容包括学习与规划方法，以及它们的统一框架（n-step 方法与 Dyna 结构）。本部分新增多个算法：**UCB、Expected Sarsa、Double Learning、Tree-backup、Q(λ)、RTDP、MCTS** 等。  
通过从表格型入手并深入讲解，读者可在最简环境中掌握强化学习的核心思想。

**第二部分（第 9–13 章）**：  
介绍**函数逼近（function approximation）**。新增主题包括人工神经网络、Fourier 基函数、LSTD、核方法（kernel-based methods）、Gradient-TD 与 Emphatic-TD、平均奖励方法（average reward methods）、True Online TD(λ)、以及策略梯度方法（policy gradient methods）。  
此外，本版大幅扩展了**离策略学习（off-policy learning）**的讨论：先在第 5–7 章中讲述表格型，再在第 11–12 章中扩展至函数逼近。  
另一处变化是，将 **n-step 启发（forward view）** 与 **资格迹（backward view, eligibility traces）** 分离，分别在第 7 章与第 12 章独立讲述。

**第三部分（第 14–16 章）**：  
全新章节探讨强化学习与**心理学（第14章）**、**神经科学（第15章）**的关系，并更新了案例研究章节（第16章），涵盖 **Atari 游戏、IBM Watson 筹码策略、AlphaGo 与 AlphaGo Zero** 等。  
由于篇幅所限，我们只能选择性介绍部分研究，重点仍放在**无模型（model-free）方法**与可扩展的大规模应用。  
最后一章新增了**强化学习的社会影响**讨论。整体上，本版篇幅约为第一版的两倍。

---

### 🎓 教学用途与课程建议

本书可作为强化学习课程的一、两学期教材：

- 若为**一学期课程**，建议完整覆盖第 1–10 章，可根据兴趣加入相关扩展阅读，如  
  Bertsekas & Tsitsiklis (1996)、Wiering & van Otterlo (2012)、Szepesvári (2010)。  
  学生若缺乏在线监督学习背景，可补充该部分内容。  
  “选项模型（options and option models）”是自然的补充主题（Sutton, Precup & Singh, 1999）。

- 若为**两学期课程**，可覆盖全书内容及更多文献扩展。

- 本书亦可作为更广泛课程（机器学习 / 人工智能 / 神经网络）的组成部分。  
  对于时间有限的课程，我们建议：  
  - 阅读第1章概览；  
  - 第2章至2.4节；  
  - 第3章；  
  - 其余章节可按兴趣选择。  
  其中**第6章是全书最核心的一章**。  
  若课程偏向机器学习或神经网络，应重点讲第9、10章；若偏向人工智能或规划，则重点讲第8章。  
  全书中标有“*”号的章节或小节为**非核心部分**，可在初读时跳过。带“*”号的习题亦为进阶内容，可选做。

---

### 🧩 文献与历史说明

多数章节结尾附有“**文献与历史注释（Bibliographical and Historical Remarks）**”，  
用于致谢原始思想的提出者、提供进一步阅读路径、并说明相关历史背景。  
尽管我们努力使这些注释准确且全面，但仍难免遗漏部分重要工作。对此我们再次致歉，并欢迎修订与补充，以纳入电子版更新中。

---

### 🧠 致谢与纪念

与第一版一样，本书仍**献给 A. Harry Klopf 的纪念**。  
是他让我们相识，也是他关于“大脑与人工智能”的思想，开启了我们长期的强化学习探索。

Harry 受过神经生理学训练，长期关注机器智能。当时他是美国空军科研办公室（AFOSR）航空电子处的高级科学家。他对传统平衡型过程（如体内稳态、误差修正分类）解释自然智能的不满，促使他提出另一种观点：  
> “**最大化某种目标的系统（maximizing systems）与寻求平衡的系统根本不同，前者或许是理解自然智能与构建人工智能的关键。**”

他促成了 AFOSR 对相关研究的资助，该项目于 1970 年代末在麻省大学阿默斯特分校（UMass Amherst）展开，由 Michael Arbib、William Kilmer、Nico Spinelli 等教授领导，他们均为计算机与信息科学系成员及控制论神经系统中心创始人。  
Barto 当时刚从密歇根大学博士毕业，受聘为博士后；Sutton 则是斯坦福大学心理学与计算机科学本科生，与 Harry 就刺激时序在经典条件反射中的作用展开通信。  
在 Harry 的推荐下，Sutton 加入该项目，成为 Barto 的博士生。这项研究正是本书强化学习思想的起点。  
Harry 不仅将我们带到一起，也奠定了强化学习研究的科学方向。  
我们由衷感谢他对这一领域与我们合作的深远影响。  
同时感谢 Arbib、Kilmer 与 Spinelli 教授提供探索这些思想的机会。  
我们亦感谢 AFOSR 在早期研究阶段的慷慨支持，以及美国国家科学基金会（NSF）在后续多年的资助。

---

### 💬 特别致谢（第二版）

我们感谢所有在第一版中给予启发与帮助的人们；若无他们的贡献，就没有今天的第二版。  
此外，我们还要感谢许多专为第二版提供支持的朋友与学生们：

- 多年教授此课程的学生们提供了无数反馈：纠正错误、提出改进、并以他们的“困惑”提醒我们解释得不够清晰之处。  
- 特别感谢 **Martha Steenstrup** 全书精读并提出详细修改意见。  
- 心理学与神经科学章节（14、15章）的写作得益于多位专家指导。  
  感谢 **John Moore** 多年耐心指导动物学习实验与神经科学理论；  
  感谢 **Matt Botvinick、Nathaniel Daw、Peter Dayan、Yael Niv** 的深刻评论与文献指引。  
  （若这些章节仍有错误，责任完全在我们。）  
- 感谢 **Phil Thomas** 帮助我们使这些章节更易于非心理学/非神经科学读者理解；  
  感谢 **Peter Sterling** 改进文字表述；  
  感谢 **Jim Houk** 引入基底神经节信息处理研究；  
  感谢 **José Martínez、Terry Sejnowski、David Silver、Gerry Tesauro、Georgios Theocharous、Phil Thomas** 等，帮助我们理解其强化学习应用，并审阅案例章节初稿。  
  特别感谢 **David Silver** 对 **蒙特卡洛树搜索（MCTS）** 与 **DeepMind 围棋程序** 的解释与指导。  
  感谢 **George Konidaris** 协助 Fourier 基函数部分，  
  以及 **Emilio Cartoni、Thomas Cederborg、Stefan Dernbach、Clemens Rosenbaum、Patrick Taylor、Thomas Colin、Pierre-Luc Bacon** 在不同方面提供的重要帮助。

Sutton 还特别感谢阿尔伯塔大学强化学习与人工智能实验室的同事们：  
- **Rupam Mahmood** 对第5章离策略蒙特卡洛方法的关键贡献；  
- **Hamid Maei** 协助第11章离策略学习的理论框架；  
- **Eric Graves** 执行第13章实验；  
- **Shangtong Zhang** 复现并验证几乎所有实验结果；  
- **Kris De Asis** 改进第7与第12章的技术细节；  
- **Harm van Seijen**（及 **Hado van Hasselt**）提出前向视图与后向视图等价性的洞见。  

Sutton 感谢阿尔伯塔省政府与加拿大自然科学与工程研究委员会（NSERC）的长期资助，  
特别感谢 **Randy Goebel** 为强化学习研究营造远见性的学术环境。  
他还感谢 **DeepMind** 在本书最后六个月写作期间的支持。

最后，我们感谢所有在互联网上阅读本书草稿并指出问题的读者——  
你们发现了许多我们遗漏的错误，并提醒我们哪些地方仍需改进。

---

## 🧮 关键符号与术语表

| 符号 / 概念 | 含义 | 备注 |
|--------------|------|------|
| **St, At, Rt** | 时间步 *t* 的状态、动作、奖励 | 随机变量（大写） |
| **s, a, r** | 状态、动作、奖励的取值 | 实例（小写） |
| **p(s′, r | s, a)** | 联合概率：在状态 s、动作 a 下转移至 s′ 并获得奖励 r | 第二版新增记号 |
| **vπ(s)** | 策略 π 下的状态价值函数 | 小写函数形式 |
| **Q_t(s, a)** | 表格型估计的动作价值 | 大写表示 tabular 形式 |
| **w_t, x_t** | 权重与特征向量（旧 θ_t, φ_t） | 粗体小写 |
| **n-step / Eligibility Trace** | 前向视图与资格迹 | 第二版区分讲解 |

---

## 🧠 学习笔记（建议框架）

- 第二版反映出强化学习从“算法”走向“学科”的过程。  
- 强调了从 tabular → function approximation → neuroscience 的知识体系演进。  
- 推荐结合第 2–8 章的代码复现，以掌握核心算法思维。  
- Klopf 的思想可视为“最大化驱动智能”的哲学源头，对理解奖励信号本质有启发。  
- 可在实践部分设计：UCB、Expected Sarsa、Double Q-learning、Dyna、True Online TD(λ) 等实验脚本。

---

> 📘 **声明**  
> 本译文仅用于学习与研究目的，非商业用途。  
> 原著版权归 Richard S. Sutton 与 Andrew G. Barto 所有。  
> 翻译与笔记版权 © 2025 kun hong。遵循 [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) 协议。
