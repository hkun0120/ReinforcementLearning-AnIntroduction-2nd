# 第1章 引言（Chapter 1: Introduction）

## 引言
当我们思考“学习”的本质时，最先浮现在脑海里的，大概就是**通过与环境交互而学习**这一想法。婴儿在玩耍、挥动手臂或四处张望时，并没有明确的老师在指点，但他与环境之间存在直接的**传感—运动**联系。沿着这条联系不断“试一试”，就会获得关于**因果关系**、关于**行动的后果**以及**为了实现目标应该做什么**等丰富的信息。在人的一生中，这样的交互无疑是我们获得关于环境以及自我的知识的重要来源。无论是学习开车还是进行对话，我们都会敏锐地感知环境对我们行为的反应，并试图通过行为来影响正在发生的事。**从交互中学习**几乎是所有学习与智能理论的奠基性思想。

本书探讨的是一种**从交互中学习的计算途径**。我们并不直接对人或动物如何学习进行理论化，而是主要研究**理想化的学习情境**并评估各种学习方法的有效性。也就是说，我们采取一种**人工智能研究者/工程师**的视角：设计能够有效解决具有科学或经济意义的学习问题的机器，并通过**数学分析**或**计算实验**来评估这些设计。我们所要探讨的方法称为**强化学习（reinforcement learning, RL）**，与其他机器学习方法相比，它更加专注于**面向目标的交互式学习**。

---

## 1.1 强化学习（Reinforcement Learning）

**强化学习**的目标是学会“该做什么”——即如何把**情境（状态）**映射到**动作**，以**最大化某个数值型的奖励信号**。学习者不会被直接告知“在某情境下应采取何种正确动作”，而是必须通过**尝试**来发现哪些动作能带来更多奖励。在最有趣、也最具挑战性的情形中，动作不仅会影响**即时奖励**，还会影响**下一步的情境**，从而影响**之后的一系列奖励**。因此，**“试错搜索”**与**“延迟回报”**是强化学习最重要的两大特征。

像许多以 “-ing” 结尾的主题（如 machine learning、mountaineering 登山）一样，“reinforcement learning”同时指**一个问题**、**一类对该问题行之有效的解法**，以及**研究该问题与其解法的领域**。把这三者都用一个名字称呼很方便，但**概念上把“问题”与“方法”区分开**至关重要；混淆二者常是误解的根源。

我们用**动力系统理论**中的思想来形式化强化学习问题，具体地，把它描述为**在未知（或不完全已知）的马尔可夫决策过程（MDP）上进行最优控制**。细节将在第3章展开；其基本思想是：抓住现实中一个学习体与环境交互、以达成目标时所面临问题的关键要素。一个**学习体（agent）**必须能在一定程度上感知环境状态，并采取**影响状态**的动作；它还必须拥有与环境状态相关的**目标**。MDP 旨在以尽可能简单而不流于空洞的方式，包含这三项核心要素——**感知、动作、目标**。任何适合解决此类问题的方法，我们都视为**强化学习方法**。

强化学习**不同于监督学习**（现代机器学习研究中最常见的学习范式）。监督学习从一个由**外部知识监督者**提供的**带标签样本**训练集里学习：每个样本包含一个情境描述和“正确答案”（标签），例如正确的类别。其目标是让系统把学到的反应**泛化**到训练集中未出现的情境。这种学习很重要，但**单独**并不足以支持**交互式学习**。在交互式问题中，很难获得**既正确又覆盖面充分**的“期望行为示例”，尤其在“无人区”（恰恰最需要学习之处），智能体必须**从自己的经验中学习**。

强化学习也**不同于无监督学习**（通常旨在从无标签数据中发现结构）。虽然强化学习不依赖“正确示例”，容易被误以为是无监督学习的一种，但 RL 的目标是**最大化奖励信号**，而非发现潜在结构。发现结构可以对 RL 有帮助，但它本身并不能解决“最大化奖励”这一核心目标。因此，我们把强化学习看作**第三种机器学习范式**，与监督、无监督并列（以及可能的其他范式）。

强化学习中特有的一个挑战是**探索—利用（exploration–exploitation）权衡**。为了获取大量奖励，智能体应倾向于选择**过去试过且被证明有效**的动作（利用）；但为了发现这些动作，它又必须去尝试**尚未选择过**的动作（探索）。若只探索不利用或只利用不探索，都会失败。特别是在**随机任务**中，每个动作都需**多次尝试**，才能可靠估计其期望奖励。探索—利用难题虽已被数学家研究了数十年，仍未有“终极解决”。而且，这一难题在纯粹的监督/无监督范式中根本不会出现。

强化学习的另一个关键特征在于：它**明确处理“目标导向的智能体在不确定环境中交互”的完整问题**，而不仅是其中某个子问题。与之对照，许多方法只研究一个子问题而缺乏与整体的呼应。比如，有的工作研究监督学习，却没有讨论其在更大系统中的最终用途；有的研究给出“具一般目标的规划”理论，却不考虑规划在**实时决策**中的角色，或**用于规划的模型**应当从何而来。RL 采取相反路径：**从完整的、交互式、逐利（goal-seeking）的智能体出发**，其目标是明确的，它能感知环境某些方面并选择动作来影响环境，而且在一开始就假定环境存在显著不确定性。当 RL 结合规划时，它必须正面解决**规划与实时动作选择的相互作用**，以及**环境模型如何获得与改进**。当 RL 使用监督学习时，也必须因为具体目的而用，从而确定哪些能力关键、哪些不关键。要想在学习研究上取得进展，当然需要把重要子问题分离出来研究，但这些子问题应当在“完整、交互、面向目标的智能体”中有清晰的角色，即便完整智能体的所有细节尚难以填补。

所谓“完整、交互、目标导向的智能体”，并不总意味着一个“完整的生物体或机器人”。这类例子当然成立，但**一个更大系统中的组件**也可以是这样的智能体：该组件直接与更大系统的其余部分交互，并**间接**与更大系统的环境交互。一个简单例子是：监测机器人电池电量并向控制架构发出指令的模块。它的“环境”就是**机器人本体 + 机器人所处的环境**。从最直观的例子跳出来，才能体会 RL 框架的普适性。

现代强化学习令人兴奋的一点，是它与其他工程与科学学科的**深度互促**。RL 是 AI/ML 在过去几十年里**与统计、优化等数学学科加深融合**趋势的一部分。比如，某些 RL 方法与**参数化逼近器**结合，能在运筹学与控制理论中缓解经典的“**维数灾难**”。更独特的是，RL 与**心理学、神经科学**也有强互动，而且是双向受益：RL 可能是所有 ML 范式中**与人/动物学习最接近**的，许多核心算法最初就受到了生物学习系统的启发；反过来，RL 也为心理学提供了更契合实证数据的动物学习模型，并为**脑奖赏系统**提供了有影响力的计算模型。本书主体发展与**工程/AI**相关的 RL 思想，与心理/神经科学的联系见第 14、15 章。

最后，RL 也是 AI 中“回归**简单的一般性原理**”大趋势的一部分。上世纪 60 年代末以来，不少 AI 研究者认为智能并无一般原理可言，而是大量“专用技巧/启发”的总和；把“基于一般原理的方法”（如搜索或学习）称作“弱方法”，而把“基于特定知识的方法”称为“强方法”。这一观点如今并不常见。从我们的角度看，这种否定**为时过早**：在寻找一般原理方面投入的努力**远远不够**。今天的 AI 中，关于“学习、搜索、决策”的一般原理的研究已蔚然成风。钟摆会摆回多远尚难断言，但 RL 的研究无疑属于**朝着更少、更简单一般原理**回摆的力量之一。

---

## 1.2 例子（Examples）

理解强化学习的一个好办法，是看看促使其发展的一些**典型例子与潜在应用**：

- **国际象棋大师**落子时，既依赖**规划**（预想对手可能的应对与反制），也依赖对局面与走法“好坏”的**直觉判断**。  
- **自适应控制器**在石油炼厂的生产线上实时调参；它在工程师给定的边际成本框架下优化**产量/成本/质量**权衡，而不拘泥于原先的设定点。  
- **瞪羚幼崽**出生几分钟后挣扎着站立，半小时后能以每小时 20 英里的速度奔跑。  
- **移动机器人**决定是进入新房间寻找更多垃圾，还是开始返回充电站；它依据**当前电量**与以往**寻找充电器的效率**做决策。  
- **Phil 做早餐**：看似平常，却包含复杂的条件式行为与“目标—子目标”层次——去橱柜、开门、选麦片盒、伸手、抓取、取回；获取碗、勺、牛奶等都涉及眼睛快速转动来收集信息并引导手与移动；还要不断判断如何携带物品、是否先把一部分运到餐桌。每一步由具体目标引导，并服务于更上层的目标，最终是获取营养。Phil 还会（有意识或无意识地）利用身体状态信息（营养需要、饥饿程度、偏好）。

这些例子共享一些**基础且易被忽略**的特征：都涉及**能动的决策主体与环境的交互**，主体在**不确定**的环境中追求目标；主体的动作允许影响环境的未来状态（下一步棋、炼厂水库液位、机器人的下个位置与电量），进而影响随后时刻可用的动作与机会。正确选择需要考虑行动的**间接、延迟后果**，因此可能需要**前瞻/规划**。同时，动作的效果又无法被完全预测，主体必须**频繁监测环境并适时反应**（比如 Phil 倒牛奶要看着碗，避免溢出）。此外，所有例子的目标在某种意义上是**显式**的——主体能基于可感知信息来判断进展：棋手知道输赢，控制器知道产量，幼崽知道是否摔倒，机器人知道电量是否耗尽，Phil 知道早餐是否令人满意。

在这些例子中，主体都可以利用**经验提升表现**：棋手改进评估直觉、幼崽提升奔跑效率、Phil 精简做早餐流程。任务开始时，主体所具备的知识（源自以往经验或设计中内嵌）会影响“什么更容易学、什么更有用”，但**与环境的交互**对针对具体任务特征的调整至关重要。

---

## 1.3 强化学习的要素（Elements of RL）

在**智能体—环境**之外，强化学习系统通常包含四个主要子要素：**策略（policy）**、**奖励信号（reward）**、**价值函数（value function）**，以及（可选的）**环境模型（model）**。

- **策略（policy）**：描述在某一时刻**如何行为**。粗略地说，它是**从可感知状态到动作**的映射；类似心理学中的**刺激—反应**规则。策略可以是简单函数或查表，也可以内含复杂搜索；一般而言，策略可以是**随机的**，即给出各动作的选择概率。

- **奖励信号（reward）**：定义强化学习问题的**目标**。每个时间步，环境向智能体发出一个**标量奖励**。智能体的唯一目标是**长期内最大化奖励总和**。奖励划定了“好/坏”事件；在生物系统中，可以类比为“愉悦/痛苦”。奖励是调整策略的**主要依据**：若某动作后出现了低奖励，则策略可能被改以在相似情境中选其他动作。一般来说，奖励可以是**状态与动作的随机函数**。

- **价值函数（value）**：反映**长远的好坏**。一个状态的价值，大致是从该状态出发，智能体在未来能够累积到的奖励期望。奖励决定了当下的**内在可取性**，而价值考虑了未来可能到达的状态及其可获得的奖励，刻画**长期可取性**。因此我们在决策时更关注**价值**而非即时奖励——我们追求能带来**最高长期回报**的状态。价值必须根据**终身的观测序列**不断估计与再估计；几乎所有 RL 算法的核心都在于**高效的价值估计**。

- **环境模型（model）**（可选）：用来**模拟/推断**环境行为。给定状态与动作，模型可以预测**下一状态与奖励**。借助模型可以进行**规划（planning）**——在实际经历之前，对可能的未来情境进行推演并据此决策。使用模型与规划的方法称为**基于模型（model-based）**；与之相对的是**无模型（model-free）**方法，强调**直接试错**。第8章将讨论同时进行试错学习、模型学习与基于模型规划的系统。现代 RL 覆盖从低层试错到高层深思熟虑的规划的**全谱系**。

---

## 1.4 局限与范围（Limitations and Scope）

RL 大量依赖**“状态”**这一概念：它是策略与价值函数的输入，也是模型的输入与输出。非正式地，可以把**状态**理解为向智能体传达某一时刻“环境**如何**”的信息。本书严格的状态定义基于第3章提出的 MDP 框架。更广泛地，我们鼓励读者沿用直观含义：状态就是智能体可获得的关于环境的**任何信息**。我们假设状态信号由某个**预处理系统**产生，该系统名义上属于环境的一部分。本书并不深入讨论**状态构造/变更/学习**（第17.3节有简述），并非因为其不重要，而是为了把注意力集中在**决策**问题上。换言之，我们关注的是：**给定某种状态信号，如何选择动作**。

本书大多数 RL 方法围绕**价值函数估计**展开，但解决 RL 问题并**不一定**要估值。例如，**遗传算法、遗传规划、模拟退火**等优化方法不会显式估值。它们在多个静态策略上并行评估，每个策略与独立环境实例交互较长时间，**获得最多回报**的策略及其随机变体保留到下一代。我们将其称为**进化方法**：它们的工作方式类似生物进化，即便个体在其寿命内并不学习。若策略空间足够小或结构良好，或搜索时间充裕，进化方法可以有效；此外，当智能体**无法充分感知**环境状态时，它们也有优势。

但我们的焦点是**在交互中学习**的 RL 方法——这是进化方法不具备的。能够利用**个体行为交互细节**的方法，在很多情况下比进化方法**更高效**。进化方法忽略了大量 RL 问题的有用结构：它们没有利用“**策略是状态到动作的函数**”这一事实，也不关注个体在其寿命中**经历了哪些状态、选择了哪些动作**。这些信息有时会误导（例如状态被误感知），但更多时候会**提升搜索效率**。虽然“进化”与“学习”共享许多特征，且天然可协同，但我们并不认为**仅靠进化方法**特别适合 RL 问题，因此本书不作展开。

---

## 1.5 扩展示例：井字棋（Tic-Tac-Toe）

为阐明 RL 的一般思想并与其他方法做对比，我们更详细地考察一个例子：**井字棋**。棋盘为 3×3，双方轮流下 **X** 与 **O**，横/竖/斜连成三子者胜；若棋盘下满仍无人连成三子则平局。由于熟练者可做到**不败**，我们假设对手并不完美，偶尔犯错使我们有机会取胜。也暂且把**平局与失败**视作同样“不好”。如何构造一个能**抓住对手瑕疵并最大化胜率**的玩家？

经典技术并不好直接处理此题。比如，博弈论的**极小化极大（minimax）**方案假设了对手的一种特定下法；它不会走向“**可能导致输**”的局面，即便在实际对手的错误下，从那个局面我们总能赢。动态规划（DP）在理论上可以针对**任何对手**算出最优解，但它需要**完整的对手模型**（在每个局面以何概率走每步），这在现实里几乎从来不可用。不过，这类信息可以**通过对弈经验估计**出来：先学对手，再用 DP 求最优——这与我们将在书中介绍的一些 RL 方法并无本质不同。

**进化方法**会直接在**策略空间**中搜索能高胜率的策略。策略就是在每个棋盘局面下“下哪步”的规则。评估策略时，反复与对手对弈若干盘，以**胜率**估计其好坏，并据此引导下一步搜索（爬山、遗传等）。优化方法可谓“百家争鸣”。

**基于价值函数**的方法如下：为**每个可能的局面**设置一个表项，记录“从该局面出发我们最终**获胜的概率估计**”，即该局面的**价值**。若我们总下 **X**，那么凡是“已有三连 X”的局面价值为 1；“已有三连 O”或“棋盘已满”的局面价值为 0；其余初始为 0.5（胜率五五开）。对弈时，对每个可落子的空位，**假设**落子后得到的后继局面，查表取其当前价值。大多数时候**贪心**选择**价值最高**的落子；偶尔也会随机选择其他落子（**探索**），以经历否则永远遇不到的局面。对弈中的“考虑过/实际走”的序列可如图 1.1 所示（此处略图）。

在对弈过程中，我们会**更新**所经历局面的价值，使其更接近“真实胜率”。具体地，在每次**贪心**落子后，把**后继局面的价值**“回传”到**前一局面**：令前一局面的价值**朝向**后一局面的价值移动一小步。若用 \(S_t\) 表示贪心落子前的局面，\(S_{t+1}\) 为落子后的局面，其估计价值 \(V(S)\) 的更新为：
\[
V(S_t) \leftarrow V(S_t) + \alpha \bigl(V(S_{t+1}) - V(S_t)\bigr),
\]
其中 \(\alpha>0\) 为**步长（学习率）**。该更新是一种**时序差分（TD）学习**：依据相邻两时刻的两个估计值之差进行调整。

若随时间**恰当递减**步长，上述方法对于任一固定对手都将**收敛**到“在我们的最优博弈下，从每个局面获胜的真实概率”。此时（除探索步外）所选走法即为对该对手的**最优策略**。若步长并未衰减到 0，则此玩家还能**适应**那些**缓慢改变**下法的对手。

这个例子也凸显了**进化方法**与**价值函数方法**的差异：前者为评估策略，会把策略**固定**，然后打很多盘；只用**最终胜负**来评估，而**忽略对局过程中的丰富信息**（哪些关键步导致了胜、甚至给从未下过的步“分功”）。价值函数方法则允许**逐局面地评估**，更好地利用过程信息。两者最终都在“策略空间”里搜索，但**学习价值函数**能更充分利用“对弈过程”的信息。

同时，这个例子虽简单，却容易给人一种误解：RL 的适用范围很有限。事实上，RL 完全可以应用于**对手缺席**的“与自然博弈”；也不限于“分回合、有终止、末端才给奖励”的问题；它同样适用于**持续的、任意时刻给出不同幅度奖励**的问题；甚至可处理**连续时间**情形（但理论更复杂，本书不展开）。井字棋的状态空间虽小且有限，但 RL 也能应对**巨大甚至无穷**的状态空间。例如 Tesauro（1992, 1995）将上述算法与**神经网络**结合，学习西洋双陆棋（状态数约 \(10^{20}\)）。由于状态极多，不可能“充分经历”，神经网络在此起到**泛化**作用：面对新局面，依据过去相似局面的经验做决策。RL 在大状态问题中能否奏效，**与其泛化能力密切相关**；在这个角色中，监督学习方法在 RL 内部尤为重要（深度学习并非唯一或必然最佳方案）。

需要强调，RL **并不等同于“白板起步”**。先验信息可以通过多种方式融入 RL，对**高效学习**可能至关重要（参见第 9.5、13.1、17.4 节）。此外，井字棋中我们能够访问**真实状态**；但 RL 也适用于**部分可观测**或**混淆**状态的情形。

最后，井字棋玩家在评估落子时，**向前看**并知道每一步可能导致的局面——这要求有一个**环境模型**。许多问题具备这样的短期模型，但也有许多问题连短期模型都没有。RL 在两种情况下都可以使用：**无模型**不是障碍；若模型可用或可学，也可轻松利用（第8章）。在某些问题上，**无模型方法**甚至更有优势，因为**构建足够准确的模型**才是瓶颈；而且，无模型方法也是**基于模型**方法的重要**构件**。

RL 可用于系统中的高层或低层。虽然在井字棋例子里，我们只学习基本走法，但完全可以在**更高层**使用 RL：每个“动作”本身可以是一段复杂问题求解过程；在**层级学习**系统中，RL 可以**多层并行**工作。

**练习 1.1 自博弈（★）**  
如果玩家**自对弈**（两方同时学习），会发生什么？学到的策略会不同吗？

**练习 1.2 对称（★）**  
许多井字棋局面看起来不同，实则**对称等价**。如何修改上述学习过程来**利用对称性**？会带来哪些改进？再想一想：如果对手**不**利用对称性，我们是否还应当利用？对称等价的局面**必然**应赋予相同价值吗？

**练习 1.3 贪心博弈（★）**  
若玩家**始终贪心**（总选当前估计最佳步），会比“非贪心”学得更好还是更差？可能出现什么问题？

**练习 1.4 从探索中学习（★）**  
如果**探索步**之后也进行学习更新（且步长随时间恰当减小，但**探索倾向不减**），状态价值将收敛到**另一组**概率。概念上，分别在“**不**从探索步学习”与“**从**探索步学习”两种情形下，价值在估计**哪两类概率**？若继续保持探索，学哪一类更好？哪一类带来更多胜局？

**练习 1.5 其他改进（★）**  
能想到其他改进该玩家的方法吗？有更好的方式解决本文所述的井字棋问题吗？

---

## 1.6 小结（Summary）

**强化学习**是一种用于理解与自动化**目标导向的学习与决策**的计算方法。它强调**智能体通过与环境的直接交互学习**，不要求完备监督或完备模型。我们认为，RL 是**第一个**严肃处理“从与环境交互中学习以实现**长期目标**”所产生的计算问题的领域。

RL 使用**马尔可夫决策过程（MDP）**这一框架，来刻画智能体与环境之间在**状态、动作、奖励**层面的交互，这是一种简明表达 AI 问题基本特征（因果、不确定性/随机性、显式目标）的方法。

**价值与价值函数**是本书多数 RL 方法的关键。我们的立场是：价值函数有助于在**策略空间**中进行高效搜索；这也使 RL 与**直接在策略空间全局搜索**（仅以整策略评估为依据）的**进化方法**区分开来。

---

## 1.7 强化学习早期史（Early History of RL）

RL 的早期历史有两条主要且丰富的脉络，最初相互独立，后在现代 RL 中交织：  
（1）**试错学习（trial-and-error）**——源于**动物学习心理学**，经由早期 AI 研究，最终引发 1980s 的 RL 复兴；  
（2）**最优控制及其以价值函数与动态规划求解**——这一路线大多**不涉及学习**。  
此外，还有一条较模糊但重要的脉络：**时序差分（TD）方法**，正如本章井字棋示例中所用。三条线索在 1980s 末汇流，形成我们今日所述的 RL 领域。

**最优控制**一词出现在 1950s 末，目标是在**动力系统**上设计控制器，使某种**性能度量**最小/最大。Bellman 等人在 1950s 中期基于 Hamilton–Jacobi 理论发展出一套方法：引入**状态**与**价值函数（最优回报函数）**，提出现称**贝尔曼方程**的泛函方程；通过求解它来解决最优控制问题的方法称为**动态规划（DP）**（Bellman, 1957a）。Bellman（1957b）也提出了**离散随机**版本——**马尔可夫决策过程（MDP）**；Howard（1960）发明了**策略迭代**。这些都是现代 RL 理论与算法的基石。

DP 被广泛认为是求解一般**随机最优控制**问题的唯一可行通法，尽管它受“**维数灾难**”困扰（复杂度随状态变量数呈指数增长），但仍比其他通法更高效且适用范围更广。此后 DP 得到了大量发展（部分可观测 MDP、应用、近似、异步算法等），相关综述与教材众多。长期以来，**最优控制/DP 与学习**之间的联系并未被充分认识，学科分野、目标差异以及“DP 依赖准确模型、离线反推”的成见都可能是原因之一。直到 Watkins（1989）提出 **Q-learning**，并以 MDP 形式系统处理 RL，二者才得到充分融合；Bertsekas & Tsitsiklis（1996）将“DP + 神经网络”称为**神经动态规划**，也称**近似动态规划**——本质上都与 RL 志同道合，致力于绕开 DP 的经典短板。

从我们的广义定义看，凡是**有效解决 RL 问题**的方法都可称为 RL 方法；而 RL 与**随机最优控制（MDP）**密切相关，因此 **DP 等控制方法**也可视为 RL 的一部分。虽然传统方法几乎都要求**完整系统知识**，看起来“不像学习”，但许多 DP 算法其实是**增量/迭代**的，通过连续逼近得到正确答案。这与学习方法在形式上已非常接近；“完全知情”与“部分/未知”两种情形下的理论与解法联系如此紧密，以至于我们认为它们属于**同一主题**。

回到**试错学习**这一脉络：其思想最早可追溯到 19 世纪动物学习心理学；Thorndike（1911）提出了著名的**效果律（Law of Effect）**：若同一情境下的多个反应中，某些反应后伴随“满意”，它们与情境的联结会加强；若伴随“不适”，联结会削弱，且强度与满意/不适程度相关。这一“基于结果的强化/削弱”原则被广泛视为许多行为的基础，并影响了 Hull 与 Skinner 的理论与实验方法。“强化（reinforcement）”一词在动物学习语境中出现较晚（可追溯到 Pavlov 条件反射的英译本）。Turing（1948）在早期 AI 思考中就提出了类似“**快感—痛苦系统**”的设想：对未定之事做随机选择，若带来“痛苦”则取消，若带来“快感”则固化。

20 世纪中叶，诸多电机/继电器装置以**试错**演示学习；随后逐步转向**数字计算机**。然而，1960–70s 的多数学习研究转向了**监督学习**（模式识别/感知学习），强化学习在相当一段时间内**式微**，只有少数例外（如 Michie 的 MENACE 井字棋、BOXES 平衡杆，Widrow 等“带评论家（critic）”的 LMS 变体，学习自动机等）。学习自动机研究直接影响了 RL 中的**k 臂老虎机**问题（第2章）；心理学中的**统计学习理论**也提供了启发。

另一条重要线索是 **时序差分（TD）学习**。其思想与心理学中“**次级强化**”有关：先与初级强化（食物/痛苦）配对的刺激也会获得强化性质。Samuel（1959）的西洋跳棋程序首次提出并实现了带**TD 思想**的学习方法；Minsky（1961）在“Steps Toward AI”中系统讨论了此事，并把它与“次级强化”关联起来。Klopf（1970s–1980s）把试错与“**广义强化**”（局部子系统之间相互强化）联系起来；Sutton 与 Barto 在 1980s 形成了基于 TD 的**心理模型**与**actor–critic**结构；Sutton（1988）把**预测**与**控制**中的 TD 概念区分开，并提出 **TD(λ)**。更早的 Witten（1977, 1976）已在 MDP 控制中提出了我们今天称为**表格 TD(0)** 的方法，承前启后地把**试错**与**最优控制**两线相连。1989 年 Watkins 的 **Q-learning** 则把三条线索汇聚到一起；随后 Tesauro 的 **TD-Gammon** 更将 RL 推向前台。

自第一版出版后，神经科学中涌现出一个蓬勃的子领域，聚焦于“RL 算法与神经系统中的学习”之间的关系。尤其是 **TD 误差**与**多巴胺神经元放电**之间的惊人相似，引发了一系列工作（见第15章）。更多历史与文献，分散在各章的“参考与历史注记”部分。

---

**注**：  
- 本译文仅用于学习与研究，非商业用途；原著版权归 Sutton & Barto 所有。  
- 公式与术语力求与原书一致；个别处为行文自然作了轻微意译与排版优化。
# 第1章 引言（Chapter 1: Introduction）

## 引言
当我们思考“学习”的本质时，最先浮现在脑海里的，大概就是**通过与环境交互而学习**这一想法。婴儿在玩耍、挥动手臂或四处张望时，并没有明确的老师在指点，但他与环境之间存在直接的**传感—运动**联系。沿着这条联系不断“试一试”，就会获得关于**因果关系**、关于**行动的后果**以及**为了实现目标应该做什么**等丰富的信息。在人的一生中，这样的交互无疑是我们获得关于环境以及自我的知识的重要来源。无论是学习开车还是进行对话，我们都会敏锐地感知环境对我们行为的反应，并试图通过行为来影响正在发生的事。**从交互中学习**几乎是所有学习与智能理论的奠基性思想。

本书探讨的是一种**从交互中学习的计算途径**。我们并不直接对人或动物如何学习进行理论化，而是主要研究**理想化的学习情境**并评估各种学习方法的有效性。也就是说，我们采取一种**人工智能研究者/工程师**的视角：设计能够有效解决具有科学或经济意义的学习问题的机器，并通过**数学分析**或**计算实验**来评估这些设计。我们所要探讨的方法称为**强化学习（reinforcement learning, RL）**，与其他机器学习方法相比，它更加专注于**面向目标的交互式学习**。

---

## 1.1 强化学习（Reinforcement Learning）

**强化学习**的目标是学会“该做什么”——即如何把**情境（状态）**映射到**动作**，以**最大化某个数值型的奖励信号**。学习者不会被直接告知“在某情境下应采取何种正确动作”，而是必须通过**尝试**来发现哪些动作能带来更多奖励。在最有趣、也最具挑战性的情形中，动作不仅会影响**即时奖励**，还会影响**下一步的情境**，从而影响**之后的一系列奖励**。因此，**“试错搜索”**与**“延迟回报”**是强化学习最重要的两大特征。

像许多以 “-ing” 结尾的主题（如 machine learning、mountaineering 登山）一样，“reinforcement learning”同时指**一个问题**、**一类对该问题行之有效的解法**，以及**研究该问题与其解法的领域**。把这三者都用一个名字称呼很方便，但**概念上把“问题”与“方法”区分开**至关重要；混淆二者常是误解的根源。

我们用**动力系统理论**中的思想来形式化强化学习问题，具体地，把它描述为**在未知（或不完全已知）的马尔可夫决策过程（MDP）上进行最优控制**。细节将在第3章展开；其基本思想是：抓住现实中一个学习体与环境交互、以达成目标时所面临问题的关键要素。一个**学习体（agent）**必须能在一定程度上感知环境状态，并采取**影响状态**的动作；它还必须拥有与环境状态相关的**目标**。MDP 旨在以尽可能简单而不流于空洞的方式，包含这三项核心要素——**感知、动作、目标**。任何适合解决此类问题的方法，我们都视为**强化学习方法**。

强化学习**不同于监督学习**（现代机器学习研究中最常见的学习范式）。监督学习从一个由**外部知识监督者**提供的**带标签样本**训练集里学习：每个样本包含一个情境描述和“正确答案”（标签），例如正确的类别。其目标是让系统把学到的反应**泛化**到训练集中未出现的情境。这种学习很重要，但**单独**并不足以支持**交互式学习**。在交互式问题中，很难获得**既正确又覆盖面充分**的“期望行为示例”，尤其在“无人区”（恰恰最需要学习之处），智能体必须**从自己的经验中学习**。

强化学习也**不同于无监督学习**（通常旨在从无标签数据中发现结构）。虽然强化学习不依赖“正确示例”，容易被误以为是无监督学习的一种，但 RL 的目标是**最大化奖励信号**，而非发现潜在结构。发现结构可以对 RL 有帮助，但它本身并不能解决“最大化奖励”这一核心目标。因此，我们把强化学习看作**第三种机器学习范式**，与监督、无监督并列（以及可能的其他范式）。

强化学习中特有的一个挑战是**探索—利用（exploration–exploitation）权衡**。为了获取大量奖励，智能体应倾向于选择**过去试过且被证明有效**的动作（利用）；但为了发现这些动作，它又必须去尝试**尚未选择过**的动作（探索）。若只探索不利用或只利用不探索，都会失败。特别是在**随机任务**中，每个动作都需**多次尝试**，才能可靠估计其期望奖励。探索—利用难题虽已被数学家研究了数十年，仍未有“终极解决”。而且，这一难题在纯粹的监督/无监督范式中根本不会出现。

强化学习的另一个关键特征在于：它**明确处理“目标导向的智能体在不确定环境中交互”的完整问题**，而不仅是其中某个子问题。与之对照，许多方法只研究一个子问题而缺乏与整体的呼应。比如，有的工作研究监督学习，却没有讨论其在更大系统中的最终用途；有的研究给出“具一般目标的规划”理论，却不考虑规划在**实时决策**中的角色，或**用于规划的模型**应当从何而来。RL 采取相反路径：**从完整的、交互式、逐利（goal-seeking）的智能体出发**，其目标是明确的，它能感知环境某些方面并选择动作来影响环境，而且在一开始就假定环境存在显著不确定性。当 RL 结合规划时，它必须正面解决**规划与实时动作选择的相互作用**，以及**环境模型如何获得与改进**。当 RL 使用监督学习时，也必须因为具体目的而用，从而确定哪些能力关键、哪些不关键。要想在学习研究上取得进展，当然需要把重要子问题分离出来研究，但这些子问题应当在“完整、交互、面向目标的智能体”中有清晰的角色，即便完整智能体的所有细节尚难以填补。

所谓“完整、交互、目标导向的智能体”，并不总意味着一个“完整的生物体或机器人”。这类例子当然成立，但**一个更大系统中的组件**也可以是这样的智能体：该组件直接与更大系统的其余部分交互，并**间接**与更大系统的环境交互。一个简单例子是：监测机器人电池电量并向控制架构发出指令的模块。它的“环境”就是**机器人本体 + 机器人所处的环境**。从最直观的例子跳出来，才能体会 RL 框架的普适性。

现代强化学习令人兴奋的一点，是它与其他工程与科学学科的**深度互促**。RL 是 AI/ML 在过去几十年里**与统计、优化等数学学科加深融合**趋势的一部分。比如，某些 RL 方法与**参数化逼近器**结合，能在运筹学与控制理论中缓解经典的“**维数灾难**”。更独特的是，RL 与**心理学、神经科学**也有强互动，而且是双向受益：RL 可能是所有 ML 范式中**与人/动物学习最接近**的，许多核心算法最初就受到了生物学习系统的启发；反过来，RL 也为心理学提供了更契合实证数据的动物学习模型，并为**脑奖赏系统**提供了有影响力的计算模型。本书主体发展与**工程/AI**相关的 RL 思想，与心理/神经科学的联系见第 14、15 章。

最后，RL 也是 AI 中“回归**简单的一般性原理**”大趋势的一部分。上世纪 60 年代末以来，不少 AI 研究者认为智能并无一般原理可言，而是大量“专用技巧/启发”的总和；把“基于一般原理的方法”（如搜索或学习）称作“弱方法”，而把“基于特定知识的方法”称为“强方法”。这一观点如今并不常见。从我们的角度看，这种否定**为时过早**：在寻找一般原理方面投入的努力**远远不够**。今天的 AI 中，关于“学习、搜索、决策”的一般原理的研究已蔚然成风。钟摆会摆回多远尚难断言，但 RL 的研究无疑属于**朝着更少、更简单一般原理**回摆的力量之一。

---

## 1.2 例子（Examples）

理解强化学习的一个好办法，是看看促使其发展的一些**典型例子与潜在应用**：

- **国际象棋大师**落子时，既依赖**规划**（预想对手可能的应对与反制），也依赖对局面与走法“好坏”的**直觉判断**。  
- **自适应控制器**在石油炼厂的生产线上实时调参；它在工程师给定的边际成本框架下优化**产量/成本/质量**权衡，而不拘泥于原先的设定点。  
- **瞪羚幼崽**出生几分钟后挣扎着站立，半小时后能以每小时 20 英里的速度奔跑。  
- **移动机器人**决定是进入新房间寻找更多垃圾，还是开始返回充电站；它依据**当前电量**与以往**寻找充电器的效率**做决策。  
- **Phil 做早餐**：看似平常，却包含复杂的条件式行为与“目标—子目标”层次——去橱柜、开门、选麦片盒、伸手、抓取、取回；获取碗、勺、牛奶等都涉及眼睛快速转动来收集信息并引导手与移动；还要不断判断如何携带物品、是否先把一部分运到餐桌。每一步由具体目标引导，并服务于更上层的目标，最终是获取营养。Phil 还会（有意识或无意识地）利用身体状态信息（营养需要、饥饿程度、偏好）。

这些例子共享一些**基础且易被忽略**的特征：都涉及**能动的决策主体与环境的交互**，主体在**不确定**的环境中追求目标；主体的动作允许影响环境的未来状态（下一步棋、炼厂水库液位、机器人的下个位置与电量），进而影响随后时刻可用的动作与机会。正确选择需要考虑行动的**间接、延迟后果**，因此可能需要**前瞻/规划**。同时，动作的效果又无法被完全预测，主体必须**频繁监测环境并适时反应**（比如 Phil 倒牛奶要看着碗，避免溢出）。此外，所有例子的目标在某种意义上是**显式**的——主体能基于可感知信息来判断进展：棋手知道输赢，控制器知道产量，幼崽知道是否摔倒，机器人知道电量是否耗尽，Phil 知道早餐是否令人满意。

在这些例子中，主体都可以利用**经验提升表现**：棋手改进评估直觉、幼崽提升奔跑效率、Phil 精简做早餐流程。任务开始时，主体所具备的知识（源自以往经验或设计中内嵌）会影响“什么更容易学、什么更有用”，但**与环境的交互**对针对具体任务特征的调整至关重要。

---

## 1.3 强化学习的要素（Elements of RL）

在**智能体—环境**之外，强化学习系统通常包含四个主要子要素：**策略（policy）**、**奖励信号（reward）**、**价值函数（value function）**，以及（可选的）**环境模型（model）**。

- **策略（policy）**：描述在某一时刻**如何行为**。粗略地说，它是**从可感知状态到动作**的映射；类似心理学中的**刺激—反应**规则。策略可以是简单函数或查表，也可以内含复杂搜索；一般而言，策略可以是**随机的**，即给出各动作的选择概率。

- **奖励信号（reward）**：定义强化学习问题的**目标**。每个时间步，环境向智能体发出一个**标量奖励**。智能体的唯一目标是**长期内最大化奖励总和**。奖励划定了“好/坏”事件；在生物系统中，可以类比为“愉悦/痛苦”。奖励是调整策略的**主要依据**：若某动作后出现了低奖励，则策略可能被改以在相似情境中选其他动作。一般来说，奖励可以是**状态与动作的随机函数**。

- **价值函数（value）**：反映**长远的好坏**。一个状态的价值，大致是从该状态出发，智能体在未来能够累积到的奖励期望。奖励决定了当下的**内在可取性**，而价值考虑了未来可能到达的状态及其可获得的奖励，刻画**长期可取性**。因此我们在决策时更关注**价值**而非即时奖励——我们追求能带来**最高长期回报**的状态。价值必须根据**终身的观测序列**不断估计与再估计；几乎所有 RL 算法的核心都在于**高效的价值估计**。

- **环境模型（model）**（可选）：用来**模拟/推断**环境行为。给定状态与动作，模型可以预测**下一状态与奖励**。借助模型可以进行**规划（planning）**——在实际经历之前，对可能的未来情境进行推演并据此决策。使用模型与规划的方法称为**基于模型（model-based）**；与之相对的是**无模型（model-free）**方法，强调**直接试错**。第8章将讨论同时进行试错学习、模型学习与基于模型规划的系统。现代 RL 覆盖从低层试错到高层深思熟虑的规划的**全谱系**。

---

## 1.4 局限与范围（Limitations and Scope）

RL 大量依赖**“状态”**这一概念：它是策略与价值函数的输入，也是模型的输入与输出。非正式地，可以把**状态**理解为向智能体传达某一时刻“环境**如何**”的信息。本书严格的状态定义基于第3章提出的 MDP 框架。更广泛地，我们鼓励读者沿用直观含义：状态就是智能体可获得的关于环境的**任何信息**。我们假设状态信号由某个**预处理系统**产生，该系统名义上属于环境的一部分。本书并不深入讨论**状态构造/变更/学习**（第17.3节有简述），并非因为其不重要，而是为了把注意力集中在**决策**问题上。换言之，我们关注的是：**给定某种状态信号，如何选择动作**。

本书大多数 RL 方法围绕**价值函数估计**展开，但解决 RL 问题并**不一定**要估值。例如，**遗传算法、遗传规划、模拟退火**等优化方法不会显式估值。它们在多个静态策略上并行评估，每个策略与独立环境实例交互较长时间，**获得最多回报**的策略及其随机变体保留到下一代。我们将其称为**进化方法**：它们的工作方式类似生物进化，即便个体在其寿命内并不学习。若策略空间足够小或结构良好，或搜索时间充裕，进化方法可以有效；此外，当智能体**无法充分感知**环境状态时，它们也有优势。

但我们的焦点是**在交互中学习**的 RL 方法——这是进化方法不具备的。能够利用**个体行为交互细节**的方法，在很多情况下比进化方法**更高效**。进化方法忽略了大量 RL 问题的有用结构：它们没有利用“**策略是状态到动作的函数**”这一事实，也不关注个体在其寿命中**经历了哪些状态、选择了哪些动作**。这些信息有时会误导（例如状态被误感知），但更多时候会**提升搜索效率**。虽然“进化”与“学习”共享许多特征，且天然可协同，但我们并不认为**仅靠进化方法**特别适合 RL 问题，因此本书不作展开。

---

## 1.5 扩展示例：井字棋（Tic-Tac-Toe）

为阐明 RL 的一般思想并与其他方法做对比，我们更详细地考察一个例子：**井字棋**。棋盘为 3×3，双方轮流下 **X** 与 **O**，横/竖/斜连成三子者胜；若棋盘下满仍无人连成三子则平局。由于熟练者可做到**不败**，我们假设对手并不完美，偶尔犯错使我们有机会取胜。也暂且把**平局与失败**视作同样“不好”。如何构造一个能**抓住对手瑕疵并最大化胜率**的玩家？

经典技术并不好直接处理此题。比如，博弈论的**极小化极大（minimax）**方案假设了对手的一种特定下法；它不会走向“**可能导致输**”的局面，即便在实际对手的错误下，从那个局面我们总能赢。动态规划（DP）在理论上可以针对**任何对手**算出最优解，但它需要**完整的对手模型**（在每个局面以何概率走每步），这在现实里几乎从来不可用。不过，这类信息可以**通过对弈经验估计**出来：先学对手，再用 DP 求最优——这与我们将在书中介绍的一些 RL 方法并无本质不同。

**进化方法**会直接在**策略空间**中搜索能高胜率的策略。策略就是在每个棋盘局面下“下哪步”的规则。评估策略时，反复与对手对弈若干盘，以**胜率**估计其好坏，并据此引导下一步搜索（爬山、遗传等）。优化方法可谓“百家争鸣”。

**基于价值函数**的方法如下：为**每个可能的局面**设置一个表项，记录“从该局面出发我们最终**获胜的概率估计**”，即该局面的**价值**。若我们总下 **X**，那么凡是“已有三连 X”的局面价值为 1；“已有三连 O”或“棋盘已满”的局面价值为 0；其余初始为 0.5（胜率五五开）。对弈时，对每个可落子的空位，**假设**落子后得到的后继局面，查表取其当前价值。大多数时候**贪心**选择**价值最高**的落子；偶尔也会随机选择其他落子（**探索**），以经历否则永远遇不到的局面。对弈中的“考虑过/实际走”的序列可如图 1.1 所示（此处略图）。

在对弈过程中，我们会**更新**所经历局面的价值，使其更接近“真实胜率”。具体地，在每次**贪心**落子后，把**后继局面的价值**“回传”到**前一局面**：令前一局面的价值**朝向**后一局面的价值移动一小步。若用 \(S_t\) 表示贪心落子前的局面，\(S_{t+1}\) 为落子后的局面，其估计价值 \(V(S)\) 的更新为：
\[
V(S_t) \leftarrow V(S_t) + \alpha \bigl(V(S_{t+1}) - V(S_t)\bigr),
\]
其中 \(\alpha>0\) 为**步长（学习率）**。该更新是一种**时序差分（TD）学习**：依据相邻两时刻的两个估计值之差进行调整。

若随时间**恰当递减**步长，上述方法对于任一固定对手都将**收敛**到“在我们的最优博弈下，从每个局面获胜的真实概率”。此时（除探索步外）所选走法即为对该对手的**最优策略**。若步长并未衰减到 0，则此玩家还能**适应**那些**缓慢改变**下法的对手。

这个例子也凸显了**进化方法**与**价值函数方法**的差异：前者为评估策略，会把策略**固定**，然后打很多盘；只用**最终胜负**来评估，而**忽略对局过程中的丰富信息**（哪些关键步导致了胜、甚至给从未下过的步“分功”）。价值函数方法则允许**逐局面地评估**，更好地利用过程信息。两者最终都在“策略空间”里搜索，但**学习价值函数**能更充分利用“对弈过程”的信息。

同时，这个例子虽简单，却容易给人一种误解：RL 的适用范围很有限。事实上，RL 完全可以应用于**对手缺席**的“与自然博弈”；也不限于“分回合、有终止、末端才给奖励”的问题；它同样适用于**持续的、任意时刻给出不同幅度奖励**的问题；甚至可处理**连续时间**情形（但理论更复杂，本书不展开）。井字棋的状态空间虽小且有限，但 RL 也能应对**巨大甚至无穷**的状态空间。例如 Tesauro（1992, 1995）将上述算法与**神经网络**结合，学习西洋双陆棋（状态数约 \(10^{20}\)）。由于状态极多，不可能“充分经历”，神经网络在此起到**泛化**作用：面对新局面，依据过去相似局面的经验做决策。RL 在大状态问题中能否奏效，**与其泛化能力密切相关**；在这个角色中，监督学习方法在 RL 内部尤为重要（深度学习并非唯一或必然最佳方案）。

需要强调，RL **并不等同于“白板起步”**。先验信息可以通过多种方式融入 RL，对**高效学习**可能至关重要（参见第 9.5、13.1、17.4 节）。此外，井字棋中我们能够访问**真实状态**；但 RL 也适用于**部分可观测**或**混淆**状态的情形。

最后，井字棋玩家在评估落子时，**向前看**并知道每一步可能导致的局面——这要求有一个**环境模型**。许多问题具备这样的短期模型，但也有许多问题连短期模型都没有。RL 在两种情况下都可以使用：**无模型**不是障碍；若模型可用或可学，也可轻松利用（第8章）。在某些问题上，**无模型方法**甚至更有优势，因为**构建足够准确的模型**才是瓶颈；而且，无模型方法也是**基于模型**方法的重要**构件**。

RL 可用于系统中的高层或低层。虽然在井字棋例子里，我们只学习基本走法，但完全可以在**更高层**使用 RL：每个“动作”本身可以是一段复杂问题求解过程；在**层级学习**系统中，RL 可以**多层并行**工作。

**练习 1.1 自博弈（★）**  
如果玩家**自对弈**（两方同时学习），会发生什么？学到的策略会不同吗？

**练习 1.2 对称（★）**  
许多井字棋局面看起来不同，实则**对称等价**。如何修改上述学习过程来**利用对称性**？会带来哪些改进？再想一想：如果对手**不**利用对称性，我们是否还应当利用？对称等价的局面**必然**应赋予相同价值吗？

**练习 1.3 贪心博弈（★）**  
若玩家**始终贪心**（总选当前估计最佳步），会比“非贪心”学得更好还是更差？可能出现什么问题？

**练习 1.4 从探索中学习（★）**  
如果**探索步**之后也进行学习更新（且步长随时间恰当减小，但**探索倾向不减**），状态价值将收敛到**另一组**概率。概念上，分别在“**不**从探索步学习”与“**从**探索步学习”两种情形下，价值在估计**哪两类概率**？若继续保持探索，学哪一类更好？哪一类带来更多胜局？

**练习 1.5 其他改进（★）**  
能想到其他改进该玩家的方法吗？有更好的方式解决本文所述的井字棋问题吗？

---

## 1.6 小结（Summary）

**强化学习**是一种用于理解与自动化**目标导向的学习与决策**的计算方法。它强调**智能体通过与环境的直接交互学习**，不要求完备监督或完备模型。我们认为，RL 是**第一个**严肃处理“从与环境交互中学习以实现**长期目标**”所产生的计算问题的领域。

RL 使用**马尔可夫决策过程（MDP）**这一框架，来刻画智能体与环境之间在**状态、动作、奖励**层面的交互，这是一种简明表达 AI 问题基本特征（因果、不确定性/随机性、显式目标）的方法。

**价值与价值函数**是本书多数 RL 方法的关键。我们的立场是：价值函数有助于在**策略空间**中进行高效搜索；这也使 RL 与**直接在策略空间全局搜索**（仅以整策略评估为依据）的**进化方法**区分开来。

---

## 1.7 强化学习早期史（Early History of RL）

RL 的早期历史有两条主要且丰富的脉络，最初相互独立，后在现代 RL 中交织：  
（1）**试错学习（trial-and-error）**——源于**动物学习心理学**，经由早期 AI 研究，最终引发 1980s 的 RL 复兴；  
（2）**最优控制及其以价值函数与动态规划求解**——这一路线大多**不涉及学习**。  
此外，还有一条较模糊但重要的脉络：**时序差分（TD）方法**，正如本章井字棋示例中所用。三条线索在 1980s 末汇流，形成我们今日所述的 RL 领域。

**最优控制**一词出现在 1950s 末，目标是在**动力系统**上设计控制器，使某种**性能度量**最小/最大。Bellman 等人在 1950s 中期基于 Hamilton–Jacobi 理论发展出一套方法：引入**状态**与**价值函数（最优回报函数）**，提出现称**贝尔曼方程**的泛函方程；通过求解它来解决最优控制问题的方法称为**动态规划（DP）**（Bellman, 1957a）。Bellman（1957b）也提出了**离散随机**版本——**马尔可夫决策过程（MDP）**；Howard（1960）发明了**策略迭代**。这些都是现代 RL 理论与算法的基石。

DP 被广泛认为是求解一般**随机最优控制**问题的唯一可行通法，尽管它受“**维数灾难**”困扰（复杂度随状态变量数呈指数增长），但仍比其他通法更高效且适用范围更广。此后 DP 得到了大量发展（部分可观测 MDP、应用、近似、异步算法等），相关综述与教材众多。长期以来，**最优控制/DP 与学习**之间的联系并未被充分认识，学科分野、目标差异以及“DP 依赖准确模型、离线反推”的成见都可能是原因之一。直到 Watkins（1989）提出 **Q-learning**，并以 MDP 形式系统处理 RL，二者才得到充分融合；Bertsekas & Tsitsiklis（1996）将“DP + 神经网络”称为**神经动态规划**，也称**近似动态规划**——本质上都与 RL 志同道合，致力于绕开 DP 的经典短板。

从我们的广义定义看，凡是**有效解决 RL 问题**的方法都可称为 RL 方法；而 RL 与**随机最优控制（MDP）**密切相关，因此 **DP 等控制方法**也可视为 RL 的一部分。虽然传统方法几乎都要求**完整系统知识**，看起来“不像学习”，但许多 DP 算法其实是**增量/迭代**的，通过连续逼近得到正确答案。这与学习方法在形式上已非常接近；“完全知情”与“部分/未知”两种情形下的理论与解法联系如此紧密，以至于我们认为它们属于**同一主题**。

回到**试错学习**这一脉络：其思想最早可追溯到 19 世纪动物学习心理学；Thorndike（1911）提出了著名的**效果律（Law of Effect）**：若同一情境下的多个反应中，某些反应后伴随“满意”，它们与情境的联结会加强；若伴随“不适”，联结会削弱，且强度与满意/不适程度相关。这一“基于结果的强化/削弱”原则被广泛视为许多行为的基础，并影响了 Hull 与 Skinner 的理论与实验方法。“强化（reinforcement）”一词在动物学习语境中出现较晚（可追溯到 Pavlov 条件反射的英译本）。Turing（1948）在早期 AI 思考中就提出了类似“**快感—痛苦系统**”的设想：对未定之事做随机选择，若带来“痛苦”则取消，若带来“快感”则固化。

20 世纪中叶，诸多电机/继电器装置以**试错**演示学习；随后逐步转向**数字计算机**。然而，1960–70s 的多数学习研究转向了**监督学习**（模式识别/感知学习），强化学习在相当一段时间内**式微**，只有少数例外（如 Michie 的 MENACE 井字棋、BOXES 平衡杆，Widrow 等“带评论家（critic）”的 LMS 变体，学习自动机等）。学习自动机研究直接影响了 RL 中的**k 臂老虎机**问题（第2章）；心理学中的**统计学习理论**也提供了启发。

另一条重要线索是 **时序差分（TD）学习**。其思想与心理学中“**次级强化**”有关：先与初级强化（食物/痛苦）配对的刺激也会获得强化性质。Samuel（1959）的西洋跳棋程序首次提出并实现了带**TD 思想**的学习方法；Minsky（1961）在“Steps Toward AI”中系统讨论了此事，并把它与“次级强化”关联起来。Klopf（1970s–1980s）把试错与“**广义强化**”（局部子系统之间相互强化）联系起来；Sutton 与 Barto 在 1980s 形成了基于 TD 的**心理模型**与**actor–critic**结构；Sutton（1988）把**预测**与**控制**中的 TD 概念区分开，并提出 **TD(λ)**。更早的 Witten（1977, 1976）已在 MDP 控制中提出了我们今天称为**表格 TD(0)** 的方法，承前启后地把**试错**与**最优控制**两线相连。1989 年 Watkins 的 **Q-learning** 则把三条线索汇聚到一起；随后 Tesauro 的 **TD-Gammon** 更将 RL 推向前台。

自第一版出版后，神经科学中涌现出一个蓬勃的子领域，聚焦于“RL 算法与神经系统中的学习”之间的关系。尤其是 **TD 误差**与**多巴胺神经元放电**之间的惊人相似，引发了一系列工作（见第15章）。更多历史与文献，分散在各章的“参考与历史注记”部分。

---

**注**：  
- 本译文仅用于学习与研究，非商业用途；原著版权归 Sutton & Barto 所有。  
- 公式与术语力求与原书一致；个别处为行文自然作了轻微意译与排版优化。
